\chapter{Evaluation}
\label{eval:svm}
\section{Hardware selection}
\label{sec:eval:hw}
The moste relevant hardware feature of processors when working on instruction scheduling is the instruction pipeline.
We are specifically interested in the fact if a instruction pipeline is a in-order or out-of-order pipeline (\Cref{sec:bg:superscalar-cpu}).
In-order processors execute the instructions in the schedule that is written in the executable file.
A out-of-order processor might reschedule the instructions in hardware.

For our experiments we have selected two different types of processors, one of each type:
\begin{itemize}
    \item 
    % \subsubsection{AArch64}
    The AArch64 hardware architecture is a 64-bit ARM architecture with a in-order superscalar pipeline.
    A \ac{cpu} that implements this hardware, and that we use for our experiment is the Arm Cortex-A53.
    We use this \ac{cpu} in the RaspberryPi 3 Model B with a Ubuntu 20.04.
    \todo{Correct Ubuntu version?}
    
    \item
    % \subsubsection{\aurora}
    The second processor is the \aurora vector processor that implements a out-of-order superscalar pipeline.
    This processor is installed via a PCI-Express slot.
\end{itemize}
    
\section{Approach Validation}
Before starting to optimize a process, it is useful to validate that there is potential for any optimizations.
Therefore, we show, that different instruction schedules can indeed have different runtimes on our target hardware.
The approach of this experiment is differs from the other experiments because it is an early experiment that took place before our pipeline was developed.
\todo{Is this sentence useful /okay?}

% select longest bb per benchmark
% longest might have the most possible schedule, so more variation in the random schedules
\Cref{sec:approach:dataset} describes the selection of benchmarks from the LLVM Test Suite.
For this experiment we select one basic block per benchmark, for which we modify its instruction schedule.
For the selection of basic blocks we use the heuristic that balances between the most executions and the longest basic blocks, which is discussed in \Cref{sec:approach:basicblock:selection}.
A high number of instructions in a basic block is typically a good indicator for a high number of possible instruction schedules for that basic block. 
A high number of executions ensures that the basic block has a high impact on the runtime of the function that it contains.

% measure the function runtime
% we measure the runtime of the function (implemented llvm passes)
% talk about impact on measurements.
We executed this experiment in a early stage and did not have a basic block extraction pipeline nor means to measure the execution time of a single basic block.
Therefore, we must execute the whole benchmark with the modified instruction schedule of a single basic block.
However, measuring the runtime of the whole benchmark, includes the execution of much overhead code, that we are not interested in.
Thus, we measure the runtime of the function that contains the basic block of interest.
This corresponds to the third method in \Cref{fig:approach:runtime_scopes}.

We implement a pass for the LLVM optimizer, to measure the runtime of a single function.
\todo{Put into approach chapter?}
The pass searches for the function that contains the selected basic block.
Then, it injects calls to the timer functions of the C++ standard library (\lstinline|std::high_resolution_clock::now|).
The calls are injected at the beginning of the given function and right before the return statement.
It injects a compilation-unit-wide global variable, and stores the measurements into this variable.
In the destructor of that compilation-unit, the pass injects code to print all the measurements.

% generate 10 different random schedules 
% do that twice
To generate different different instruction schedules, we choose the simple approach of generating random schedules.
Our random instruction scheduler works on top of a basic list scheduler.
This means, that the list scheduler selects the instructions that are ready for scheduling, and our random scheduler randomly selects one of them.
This is done until no more instructions are left.
We set the seed of the random number generator for reproducibility.

We generate instruction schedules with the seeds 0-10 for each selected basic block, \ie we generate 11 instruction schedules per basic block.
The basic block of interest might execute multiple times in the measured function for reasons discussed in \Cref{sec:approach:runtime-measurement-unit}.
We choose the shortest measured runtime per benchmark run, to ensure that we use the same execution path in our measurements.
To check that the runtime measurements are reproducible, we run the each generated instruction schedule two times.

% evaluate
We have run this experiment on the two processors described in \Cref{sec:eval:hw}.
\Cref{fig:eval:rndm:aarch64} shows a selection of experiment results.
The plots show the runtimes grouped by the different seeds for the random instruction scheduler.
Runtimes that differ between two runs more than 5\% are marked as outliers and plotted in gray.
\Crefrange*{fig:eval:rndm:aarch64:a}{fig:eval:rndm:aarch64:d} show examples where different runtimes are clearly observable for different instruction schedules.
However, this was not always observable.
\Cref{fig:eval:rndm:aarch64:e} and \Cref{fig:eval:rndm:aarch64:f} show examples where no differnce in the runtime was observable.
The average coeffecient of variation over the basic blocks is 0.035.
In summary, we see that different instruction schedules can generate measurable differences in the runtime.
This means that there is potential for improvements.
\begin{figure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/Symbolics-flt-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aarch64:a}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/trisolv-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aarch64:b}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/smg2000-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aarch64:c}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/LoopRerolling-dbl-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aarch64:d}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/Shootout-matrix-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aarch64:e}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/fourinarow-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aarch64:f}
    \end{subfigure}
    \caption[Random Scheduling Experiment on AArch64]{Random Scheduling Experiment on AArch64:
    The bars show the runtime of a function with a random instruction schedule.
    The two runs of the instruction schedule are grouped together.
    Two runs that differ more than 5\% are marked as outliers and plotted in gray.}
    \label{fig:eval:rndm:aarch64}
\end{figure}

\Cref{fig:eval:rndm:aurora} shows a similar selection for the same experiment on the \aurora processor.
We can observe a similar outcome of the experiment.
However, as this processor cannot be interrupted by the \ac{os}, the runtimes are more stable between two runs.
No measurements in the whole example where marked as outliers.
The average coeffecient of variation over the basic blocks is 0.046.
In summary, we observe potential for optimizations on this processor.
\begin{figure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/Equivalencing-dbl-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aurora:a}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/uudecode-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aurora:b}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/automotive-susan-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aurora:c}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/bicg-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aurora:d}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/beamformer-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aurora:e}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/uuencode-crop.pdf}
        \caption{}
        \label{fig:eval:rndm:aurora:f}
    \end{subfigure}
    \caption[Random Scheduling Experiment on \aurora]{Random Scheduling Experiment on \aurora:
    The bars show the runtime of a function with a random instruction schedule.
    The two runs of the instruction schedule are grouped together.
    Two runs that differ more than 5\% are marked as outliers.
    However, this processor did not produce any outliers in our experiment.}
    \label{fig:eval:rndm:aurora}
\end{figure}

There are multiple possible reasons that would cause equal measurements in this experiment.
We must differntiate between reasons which mean that different instruction schedules have no effect on the runtime of the basic block and reasons that have its origin in the experiment setup.
We cannot do anything about the former.
Actually, the motivation for this experiment was to verify, that the former reasons do not dominate all the instruction schedules.
There are multiple possibilities for the latter reasons, that have ther origin in the experiment setup:
\begin{itemize}
    \item The basic block for which we manipulate the instruction schedule might have a low influence on the runtime of the function.
        We tried to minimize this effect by choosing basic blocks that are often executed.
    \item Our random instruction scheduler works on top of LLVM.
        LLVM makes, in this stage of the back-end, still use of pseudo instruction that are not represented in the binary.
        This means that schedules that we see as different schedules, might actually not differ in the binary.
    \item There are short functions with a short execution time.
        We observed few changes in the runtime when the measured execution time is below 10,000 processor cycles.
        The underlying timer of the C++ standard library might not be able to measure such short time periods.  
\end{itemize}
However, the experiment is still valid, because we show that we are able to influence the runtime by manipulating the instruction schedules.

In summary, we observe different runtimes for different instruction schedules and the results are reproducible over multiple runs.
This is not true for all basic blocks, but the goal of this experiment was to show the existence of an effect of the instruction schedule on the runtime.
These results motivate the further research on optimizing instruction schedules for these two processors.

\section{MCTS Schedule Search}
% Goal of the experiment
Before we can apply supervised learning approaches to the task of instruction scheduling, we need a dataset of instruction schedules that are better than the ones generated by the default compiler.
However, it a complex task to find these instruction schedules.
\ac{mcts} approaches have shown to perform good on tasks in which one has to find datapoints in big searchspaces.
Thus, we choose to use \ac{mcts} to generate a dataset of instruction schedules with a score that is relative to the default scheduler instruction schedule.

% What do we do in this experiment
In this experiment, we first measure the runtime of the basic block compiled with the LLVM default instruction scheduler for the given processor.
Next, we generate an instruction schedule with our \ac{mcts} model.
We compile the instruction schedule into an executable format (\Cref{sec:approach:bbisolation}) and execute it to measure the runtime of the basic block of interest (\Cref{sec:approach:datageneration:runtime_methods}).
We train the \ac{mcts} model with the score computed by \Cref{eqn:approach:mcts-score} and start the next iteration to train the \ac{mcts} model.
This way, we generate many instruction schedules per basic block, each evaluated with a score based on their execution time relative to the default instruction schedule.

% Baseline

% How many (and which) BBs do we use
To have a great number of instruction schedules available for our supervised learning methods, we select 20,032 basic blocks from the LLVM Test Suite.
We generate one \ac{mcts} model for each basic block.
For this experiment, we have selected the longest basic blocks in the dataset.
The number of executions is not relevant in this experiment because we isolate the basic block and measure their specific runtime on the target hardware.
A high length of the basic blocks ensures that we avoid trivial scheduling situations.

% Number of steps to outroll the MCTS tree
The experiment is time consuming because of the high number of basic blocks and the expensive runtime measurements.
We have to the experiment at some point.
We have seen, that in practice we get a speedup for many basic blocks after 200 iterations.
Therefore, we run the \ac{mcts} model for each basic block for 200 iterations.

% Exploration vs Exploitation balance weight
% WE DID NOT DESCRIBE THE FORMULAR NOWHERE

% Caching of schedules to detect duplicates
Due to the high cost of the compilation and runtime measurements, we try to avoid these steps, as much as possible.
The instruction schedule generation is done in two steps.
We generate the schedule in the LLVM back-end format that can still contain pseudo instructions, and the remaining steps in the LLVM back-end transform this into assembly instructions.
After the removal of pseudo instructions, it can happen that two equal assembly instruction schedules are generated from two different instruction schedules in the LLVM back-end format.
Therefore, we cache the measured runtimes with the hashes of the instruction schedules.
Whenever we already executed a instruction schedule with the same hash, we reuse their measured runtimes.

% Performance: Summary of the excel tables
For the AArch64 we were able to run this experiment on 14,217 basic blocks.
Due to some compilation errors, we got a unrealistic speed up for some basic blocks.
So, all speed ups greater than a factor of 2 are marked as outliers.
That leaves us with 14,162 valid instruction schedules for the AArch64 processor.
\Cref{tbl:eval:mcts} summarizes the results.
We find better performing instruction schedules for 54.79\% of these basic blocks.
In only 8.24\% of the basic blocks, we did not find an instruction schedule that performed least as good as the LLVM generated one.
On average, we increase the runtime performance of the basic blocks by 8.35\%.
\begin{table}
    \begin{subtable}{\textwidth}
        \centering
        \begin{tabular}{lrr}
            \toprule
            Instruction Schedule Performance & AArch64 & \aurora \\
            \midrule
            Better than baseline    & 54.79\% (7759) & 31.73\% (1349) \\
            Same as baseline        & 36.97\% (5236) & 53.00\% (2253) \\
            Worse than baseline     &  8.24\% (1167) & 15.27\%  (649) \\
            \bottomrule
        \end{tabular}
        \caption{}
        \label{tbl:eval:mcts-better-worse}
    \end{subtable}
    % \begin{subtable}{\textwidth}
    %     \centering
    %     \begin{tabular}{lrrr}
    %         \toprule
    %         & \multicolumn{3}{c}{Instruction Schedule performance compared to baseline} \\
    %         Architecture & Better & Equal & Worse \\
    %         \midrule
    %         AArch64 & 54.79\% (7759) & 36.97\% (5236) &  8.24\% (1167) \\
    %         \aurora & 31.73\% (1349) & 53.00\% (2253) & 15.27\%  (649) \\
    %         \bottomrule
    %     \end{tabular}
    %     \caption{}
    %     \label{tbl:eval:mcts-better-worse}
    % \end{subtable}
    % \begin{subtable}{\textwidth}
    %     \centering
    %     \begin{tabular}{rr}
    %         \toprule
    %         \multicolumn{2}{c}{Speed Up} \\
    %         AArch64 & \aurora \\
    %         \midrule
    %         8.35\% & 0.30\% \\
    %         \bottomrule
    %     \end{tabular}
    %     \caption{}
    % \end{subtable}
    \caption{}
    \label{tbl:eval:mcts}
\end{table}

We executed this experiment for 4,253 basic blocks on the \aurora processor.
The lower number of basic blocks is caused by hardware and time limitations.
Only two outliers are generated during this experiment, which results in 4,251 valid basic blocks.
See \Cref{tbl:eval:mcts} for the summarized results.
For this processor, our \ac{mcts} approach found better instruction schedules for 31.73\% of the basic blocks.
In 15.27\% of the basic blocks only worse instruction schedules were found by our model.
The average speed up of the basic blocks is 0.30\%.

% In-order vs OoO discussion (Speed Up vs BB length)
The results could still change in favor of the \aurora processor if we run this experiment on more basic blocks.
However, the result that the performance on this processor is worse than on the AArch64 processor was expected.
The reason is, that the \aurora is a out-of-order processor, and the AArch64 processor is a in-order processor.
Consequently, the \aurora might reschedule the instruction in hardware when it detects problems with the instruction schedule.
Thus, it does not depend on good instruction schedules as the AArch64 processor.

We showed with this experiment, that we are able to find better instruction schedules for both our selected processors.
However, our search for instruction schedules was more successful for the AArch64 processor, as does no rescheduling in hardware.

\section{Machine Learning Schedule Generation}
\subsection{Look-Up Model}
\subsection{Support Vector Regression}
\subsection{Neural Network}

Compare speedup with complexity of the problem (number of possible schedulings) vs speedup

Compare CPU Architectures, In-Order vs Out-Of-Order (\url{https://en.wikipedia.org/wiki/Out-of-order_execution})
Here, check speedup vs basic block length, to see if ooo processors perform worse on long basic blocks where it can't see too many instructions ahead

Might be interesting for the discussion: \url{http://www.irisa.fr/alf/downloads/PMA/p241-mcfarlin.pdf}

Mean vs. Median discussion in runtime measurements

\begin{itemize}
    \item Hardware
    \begin{itemize}
        \item Arm Cortex-A53
        \item NEC Aurora
    \end{itemize}
\end{itemize}



Run \ac{mcts} more iterations, because the rest of the schedules still contain many random decision and we can see, that a single decision can make a big difference.
