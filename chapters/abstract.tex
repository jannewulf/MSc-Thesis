\chapter*{Abstract}
% What are the goals?
% What are the research questions?
% What did we contribute?
% What are the zielsetzungen?
% What are the results?
With the growing number of processor architectures and specialized accelerators, compilers have become once again a critical tool for effective use of new capabilities.
% Many new compute devices came to the market in the last decade.
Compilers have hardware-dependent parts that need to be manually adjusted and optimized by experts for every target hardware.
One major step of the hardware-dependent compilation process is instruction scheduling.
Regarding their scheduling mechanisms, there are two groups of processors, in-order and out-of-order processors, where the latter can re-schedule itself during execution.

This thesis aims to evaluate if this step can be automatically optimized, especially for novel hardware.
We have multiple contributions through our work:
First, we develop a methodology for automatically creating optimized instruction scheduling policies for any hardware using data-driven approaches.
Next, we build a pipeline for automatic basic block micro-benchmark creation.
A large part of that pipeline is concerned with automatically standardizing the code coming from various C/C++ projects.
Lastly, we collect a training and test set extracted from the LLVM test suite.

Our experiments aim to compare the effectiveness of our approach on different processors.
We use an in-order AArch64 CPU (ARM Cortex-A53) and the \aurora, an out-of-order vector accelerator.
To summarize, we search for well-performing instruction schedules on a set of micro-benchmarks.
In an effort to reduce inference times, we train different supervised learning models with the results of the search approach to generate instruction schedules of high quality.
We generate instruction schedules for our dataset, which, on average, perform 8.35\% (in-order) and 0.30\% better (out-of-order) than the LLVM compiler framework.
Our supervised learning models generate instruction schedules that perform, on average, 1.38\% (in-order) better.
On out-of-order processors, we do not achieve a speedup with the supervised learning models.

\chapter*{Zusammenfassung}
