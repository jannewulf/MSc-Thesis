\chapter{Approach}
\label{sec:approach}
\IMRADlabel{methods}
\todo{State that generated schedules are checked by LLVM for correctness}

% Many novel hardware architectures, iot, accelerators, coprocessors
% Back-end part of compilers must be adjusted for each new hardware
% basic schedulers exist, but not optimized for hardware
Many new hardware architectures come to the market.
These include simple processors, used for \ac{iot} devices and single-board computers (\eg Raspberry Pi).
Other examples are accelerators like graphics cards, and vector processing cards (\eg \aurora).
The back-end part of compilers need to be adjusted for every new hardware architecture.
This includes the instruction scheduling task.
% instruction scheduling is hard
The compiler framework LLVM comes with general instruction schedulers that can work with a formal description of the given hardware architecture.
However, these instruction schedulers are not optimized for the given hardware.

% in order vs out of order
The optimization of instruction schedulers is a challenging, but important task.
Expensive chips often implement out-of-order pipelines, which means that they can reschedule instructions in hardware during execution.
Cheaper chips however, often do not have this functionality and benefit more from optimized instruction schedulers.
See \Cref{sec:bg:cpu} for details.
However, the optimization can increase the runtime of the compiled code.
Shorter runtimes also save energy, which is an important aspect for \ac{iot} devices.

% use data-driven methods to create schedulers automatically
% auto-tuning vs our approach
The optimization of instruction schedulers is a costly task.
Followingly, the objective of this thesis is to automize this task.
We use data-driven methods to optimize the decision making involved in the instruction scheduling task.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/ppt/approach_overview-crop.pdf}
    \caption[Overview of the approach]{Overview of the complete approach. 
    The process covers the selection of basic blocks, the instruction schedule generation, the execution model, the learning process and the derivation of the final scheduler.}
    \label{fig:approach:overview}
\end{figure}
In the remainder of this chapter, we present the approach used for this thesis (compare~\Cref{fig:approach:overview}).
We start with an overview of the benchmark programs we used~(\Cref{sec:approach:dataset}).
Then we discuss the importance of basic blocks for instruction scheduling in general and our approach specifically~(\Cref{sec:approach:basicblock}).
That is followed by explaining the \ac{mcts} approach~(\Cref{subsec:approach:ml:mcts}), which is used to find well performing instruction schedules.
Finally, we introduce our approach to train global models~(\Cref{subsec:approach:ml:global}) from the many \ac{mcts} models.

\section{Dataset}
\label{sec:approach:dataset}
For our experiments, we use benchmarks from the LLVM project~\cite{LLVM:CGO04} called LLVM Test Suite~\footnote{\url{https://llvm.org/docs/TestSuiteGuide.html}}.
The test suite contains benchmarks from the following categories
\begin{itemize}
    \item Single-Source --- built from a single C/C++ source file
    \item Multi-Source --- built from multiple C/C++ source files
    \item Micro-Benchmarks --- separate functions that are executed by google-benchmark
    \item Bitcode --- tests that are written in LLVM bitcode
    \item CTMark --- symbolic links to the other benchmarks to measure compilation performance
\end{itemize}
We select the 273 benchmarks from the Single-Source and Multi-Source categories for our experiments.
% \todo{Add table?}
The selected categories contain benchmarks that are compiled from a single source file or multiple source files, respectively.
The benchmarks are all written in C or C++.

\subsection{Modified Build Process}
\label{sec:approach:build_process}
The test suite comes with Makefiles and CMake configurations to automatically build all benchmarks.
However, these are not sufficient for our approach.
We need a modified build process for all the benchmarks because we are manipulating it in several cases.
Furthermore, the compilation cannot be done by a single command because not all arguments of the separate compilation steps are available in the main compiler.
We execute the front-end, optimization, and back-end compiler phases separately to have complete control over the build process.

\subsubsection{Front-End}
We have to set some compiler arguments to make the LLVM compiler front-end output LLVM \ac{ir} instead of compiling the program to an executable format.
We use the compiler front-ends \lstinline{clang}~\footnote{\url{https://clang.llvm.org/}} for C programs and \lstinline{clang++} for C++ programs, both in version 12.0.0.
However, the compiler arguments are the same for both front-ends.
We pass the following arguments to the front-ends:
\begin{itemize}
    \item \lstinline{-O0}: to prevent optimization in this first step
    \item \lstinline{-Xclang -disable-O0-optnone}: the \lstinline{-O0} flag alone also prevents optimization in later compilation stages
    \item \lstinline{-S -emit-llvm}: to emit LLVM \ac{ir} for further processing --- instead of creating the program's binary
\end{itemize}

\subsubsection{Optimizer}
The LLVM optimizer \lstinline{opt}~\footnote{\url{https://llvm.org/docs/CommandGuide/opt.html}} gives a choice to select one of the predefined optimization levels or select specific optimizer runs.
When we compile a benchmark for measuring the runtime, we only use the flag \lstinline{-O3} for optimizing the LLVM \ac{ir}.
This optimizes the code only on the \ac{ir} level, and does not interfere with the instruction scheduling.
We choose to use optimization in this step because it would probably be used in a production setting, too.
This makes the input code to our instruction scheduler more realistic to real-world situations.

The LLVM framework makes it easy to implement new optimizer passes.
We have implemented two passes.
The first pass counts how often basic blocks are executed during the execution of the benchmark.
This information is used in \Cref{sec:approach:basicblock:selection}.
The second pass measures the execution time of a specific function.
This pass is used for the experiment in \Cref{sec:eval:validation} and is described in \Cref{sec:approach:datageneration:runtime:function}.
These passes are not executed in the data-driven approaches described in section \Cref{sec:approach:ml}.

\subsubsection{Back-End}
The LLVM back-end compiler implements a instruction scheduling phase before the register allocation and one after the register allocation.
The first instruction scheduling phase is the one that we are interested in.
This first instruction scheduling phase was re-implemented in the LLVM project in the last years.
Both versions do currently work.
We decide to base our work on the old implementation because, the most target architectures still use this one.

We manipulate the back-end by passing arguments to the LLVM static compiler \lstinline{llc}~\footnote{\url{https://llvm.org/docs/CommandGuide/llc.html}}.
This program represents the back-end phase and executes the instruction scheduling.
We use the following arguments for the back-end:
\begin{itemize}
    \item \lstinline{--pre-RA-sched=}: To select our new instruction schedulers for the pre-register-allocation phase.
    \item \lstinline{--misched-cutoff=0}: To disable the new pre-register-allocation implementation.
    \item \lstinline{--disable-post-ra}: To disable the post-register-allocation scheduler, because we focus on optimizing the pre-register-allocation phase and do not want the post-register-allocation scheduler to manipulate our schedules. Thereby, we create a controlled environment.
\end{itemize}

We, further, analyze the CMake configurations and extract required compiler arguments on a per benchmark level.
We categorize the extracted arguments into front-end, optimizer, and back-end phases.
Finally, we append them to the previously described arguments in the respective compilation phase.

We also analyze the test suite for execution arguments.
For example, some benchmarks read data from files or require arguments when calling them.
To execute them, we generate a shell script that calls the compiled benchmark with the required arguments.

\section{Basic Block --- Scheduling Unit}
\label{sec:approach:basicblock}
Even though there is research on instruction scheduling on more extensive units (see \Cref{sec:rw:instruction-scheduling}), most compilers schedule instructions on a per basic block level.
This is also true for the LLVM compiler framework.
The advantage of this is that the limited scope avoids complicated situations and thus simplifies the scheduling problem.
For example, we do not have to deal with jump instructions.

Consequently, we also choose the basic block as the unit for instruction scheduling.
That means that we execute our experiments on individual basic blocks and learn to schedule individual basic blocks.
Later, we combine the models to generalize the knowledge to be applied to unseen basic blocks.
We describe this process in detail in \Cref{sec:approach:ml}.

\subsection{Selection Process}
\label{sec:approach:basicblock:selection}
Not all basic blocks provide an equal value to our experiments.
It also consumes too much time to work on all the basic blocks within the scope of this thesis (\eg compilation time, execution time, machine learning time).
Therefore, we sort our basic blocks by heuristics. 
From this list, we select as many as we need and can handle in our experiments.
We designed three heuristics which we will explain in the following paragraphs.
See \Cref{tab:approach:bb_heuristics} for an example.

\begin{table}
    \centering
    \begin{tabular}{@{}llrrr@{}}
        \toprule
        Function & Basic Block & Length & \(\#\) Executions & Product \\
        \midrule
        kernel\_floyd\_warshall\_StrictFP & for.body6 & 23 & 536,870,912 & 12,348,030,976 \\
        kernel\_floyd\_warshall & for.body6 & 23 & 536,870,912 & 12,348,030,976 \\
        print\_element & entry & 25 & 1,048,576 & 26,214,400 \\
        kernel\_floyd\_warshall\_StrictFP & for.cond4.preheader & 17 & 1,048,576 & 17,825,792 \\
        \(\cdots\) & \(\cdots\) & \(\cdots\) & \(\cdots\) & \(\cdots\) \\
        xmalloc & entry & 8 & 2 & 16 \\
        main & entry & 11 & 1 & 11 \\
        \bottomrule
    \end{tabular}
    \caption[Basic block heuristics for the floyd-warshall benchmark]
    {
        The tables shows an excerpt of the basic block heuristics (see \Cref{sec:approach:basicblock:selection}) for the floyd-warshall benchmark. 
        The basic blocks are sorted by the last heuristic, which is the product of the length and execution count. 
        From this list the table shows the top four and last two basic blocks in the benchmark.
    }
    \label{tab:approach:bb_heuristics}
\end{table}

\subsubsection{Longest Basic Blocks}
% \todo{Add note about influence of pipeline length}
% \todo{More different instructions use more functional units of the processor}
% The number of instructions in a basic block correlates with the number of possible schedules.
% Consequently, we can learn more from a basic block when there are more different scheduling situations available.
% For example, we will see more scheduling situations in a basic block containing 20 instructions than in a basic block containing only three instructions.

There are multiple reasons why we are more interested in longer basic blocks than in shorter basic blocks.
All these reasons summarize to the fact that there are more optimization possibilities with longer basic blocks.
We better exploit pipeline effects with longer basic blocks, because more instructions can potentially be executed interleaved (see \Cref{sec:bg:cpu}). 
With more instructions, there is potentially also a higher variety of instructions, which means that we can utilize more execution units on the processor in parallel.
Lastly, with short basic blocks, we only make few scheduling decisions per basic block. 
Summarized, this means that optimizations have a bigger effect on long basic blocks than on short basic blocks.
This was also shown by \citeauthor{stefanovic1997character}~\cite{stefanovic1997character}.

We use the optimizer pass described in \Cref{sec:approach:instr-count} to count the number of instructions per basic block.
From the output of that pass, we put together a list with all basic blocks and their lengths (see \Cref{tab:approach:bb_heuristics}).

\subsubsection{Most Executed Basic Blocks}
Some basic blocks are significantly more often executed than others.
For example, basic blocks in the bodies of loops run often.
A basic block in the error checking code in the beginning of a program might only execute once.
Often executed basic blocks influence the overall runtime a lot.
Hence, we are more interested in optimizing these.
We count the number of executions with the LLVM optimizer pass described in \Cref{sec:approach:pass-count}.
See \Cref{tab:approach:bb_heuristics} for an example.


\subsubsection{Most Executed and Longest Basic Blocks}
% \todo{Include discussion about variance in resulting basic blocks}
% \todo{most influence on overall runtime}
The previous two heuristics are helpful on their own, but we are most interested in basic blocks that combine the two aspects.
For example, the most executed basic block can still come from a very simple loop with few instructions in the loop body.
We compute the product of the two heuristics to get a combined heuristic.

\section{LLVM Compiler Passes}
The LLVM compiler framework makes it easy to implement compiler passes, which are functions that are executed during compilation and can modify the \ac{ir} code.
We implemented two of these functions, one to count the number of executions of basic blocks and one to measure the runtime of a given function.

\subsection{Basic Block Execution Counting Pass}
\label{sec:approach:pass-count}
We implement an LLVM optimizer pass for injecting counters into a program.
Optimizer passes take LLVM \ac{ir} files as input and output the code in the same file format.
Our pass injects one global 64 bit counter variable for each basic block into the code.
Next, the pass adds instructions into the beginning of each basic block, which increases the corresponding counter by one.
At the end of the main function, the pass adds a format string and injects a call to the \lstinline{printf} function to print all the counter values and corresponding basic block names.

The optimizer pass must be compiled with the LLVM framework.
We include the optimizer pass into our build process (see \Cref{sec:approach:build_process}).
There, we add a call to the LLVM optimizer \lstinline{opt} with the argument \mbox{\lstinline{-passes=dyn-bb-count}}.
To actually get the counts, we build a benchmark with this optimizer pass and execute it.
The benchmark will then print the counter values.

\subsection{Basic Block Instruction Count Pass}
\label{sec:approach:instr-count}
In order to get the number of instructions for all the basic blocks in a program we implement another pass.
This pass just prints the name of the basic block and its number of instructions to the command line during compile time.
It does not modify the code.

\subsection{Function Runtime Measure Pass}
Lastly, we implement a pass, to measure the runtime of a single function.
The pass searches for the function that contains the selected basic block.
Then, it injects calls to the timer functions of the C++ standard library (\lstinline|std::high_resolution_clock::now|).
The calls are injected at the beginning of the given function and right before the return statement.
Additionally, it injects a compilation-unit-wide global variable and stores the measurements into this variable.
In the destructor of that compilation-unit, the pass injects code to print all the measurements.

\section{Feasibility Study}
\label{sec:eval:validation}
Before starting to optimize a process, it is useful to determine whether there is potential for any optimizations.
Therefore, we show, that different instruction schedules can indeed lead to different runtimes on our target hardware.
This early experiment is also helpful to get a overview of how much the runtime can vary between different instruction schedules.

% select longest bb per benchmark
% longest might have the most possible schedule, so more variation in the random schedules
\Cref{sec:approach:dataset} describes the selection of benchmarks from the LLVM Test Suite.
For this experiment, we select one basic block per benchmark, whose instruction schedule we modify.
For the selection of basic blocks we use the heuristic that balances between the most executions and the longest basic blocks, which is discussed in \Cref{sec:approach:basicblock:selection}.

% measure the function runtime
% we measure the runtime of the function (implemented llvm passes)
% talk about impact on measurements.
We executed this experiment in a early stage and did not have a basic block extraction pipeline nor means to measure the execution time of a single basic block.
Therefore, we must execute the whole benchmark with the modified instruction schedule of a single basic block.
However, measuring the runtime of the whole benchmark includes the execution of much overhead code that we are not interested in.
Thus, we measure the runtime of the function that contains the basic block of interest instead.
This corresponds to the third method in \Cref{fig:approach:runtime_scopes}.

% generate 10 different random schedules 
% do that twice
We choose the simple approach of generating random schedules.
Our random instruction scheduler works on top of a basic list scheduler.
This means, out of the issue-ready instructions provided by the list scheduler, our scheduler chooses one at random.
This is repeated until no more instructions are left.
We fix the seed of the random number generator for reproducibility.

We generate instruction schedules with the seeds 0-10 for each selected basic block.
The basic block of interest might execute multiple times in the measured function for reasons discussed in \Cref{sec:approach:runtime-measurement-unit}.
We choose the shortest measured runtime per benchmark run to ensure that we use the same execution path in our measurements.
To check that the runtime measurements are reproducible, we run each generated instruction schedule two times.

% evaluate
We choose to carry out this experiment on a processor with the architecture AArch64 and on the \aurora.
The choice is explained in \Cref{sec:eval:hw}.
\Cref{fig:eval:rndm:aarch64} shows a representative selection of experimental results for the AArch64 processor.
The plots show the runtimes grouped by the different seeds for the random instruction scheduler.
Runtimes that differ between two runs more than 5\% are marked as outliers and plotted in gray.
\Crefrange*{fig:eval:rndm:aarch64:a}{fig:eval:rndm:aarch64:d} show examples where different runtimes are clearly observable for different instruction schedules.
However, this was not always observable.
\Cref{fig:eval:rndm:aarch64:e} and \Cref{fig:eval:rndm:aarch64:f} show examples where no difference in the runtime was observable.
% The average coeffecient of variation over the basic blocks is 0.035.
In summary, we see that different instruction schedules can generate measurable differences in the runtime.
This means that there is potential for improvements.
\begin{figure}
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aarch64:a}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/Symbolics-flt-crop.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aarch64:b}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/trisolv-crop.pdf}
    \end{subfigure}

    \vspace{0.5cm}
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aarch64:c}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/smg2000-crop.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aarch64:d}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/LoopRerolling-dbl-crop.pdf}
    \end{subfigure}

    \vspace{0.5cm}
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aarch64:e}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/Shootout-matrix-crop.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aarch64:f}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-pi-collected/fourinarow-crop.pdf}
    \end{subfigure}

    \caption[Random Scheduling Experiment on AArch64]{Random Scheduling Experiment on AArch64:
    The bars show the runtime of a function with a random instruction schedule.
    The two runs of the instruction schedule are grouped together.
    The distance measure represents the longest execution time relative to the shortest execution time.
    Two runs that differ more than 5\% are marked as outliers and plotted in gray.}
    \label{fig:eval:rndm:aarch64}
\end{figure}

\Cref{fig:eval:rndm:aurora} shows a similar selection for the same experiment on the \aurora processor.
We can observe a similar outcome of the experiment.
However, as this processor cannot be interrupted by the \ac{os}, the runtimes are more stable between two runs.
No measurements in the whole example where marked as outliers.
% The average coeffecient of variation over the basic blocks is 0.046.
In summary, we observe potential for optimizations on this processor, as well.
\begin{figure}
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aurora:a}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/Equivalencing-dbl-crop.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aurora:b}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/uudecode-crop.pdf}
    \end{subfigure}

    \vspace{0.5cm}
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aurora:c}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/automotive-susan-crop.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aurora:d}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/bicg-crop.pdf}
    \end{subfigure}

    \vspace{0.5cm}
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aurora:e}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/beamformer-crop.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.0325\textwidth}\caption{}\label{fig:eval:rndm:aurora:f}\end{subfigure}
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{img/random-scheduling-experiment-aurora-collected/uuencode-crop.pdf}
    \end{subfigure}

    \caption[Random Scheduling Experiment on \aurora]{Random Scheduling Experiment on \aurora:
    The bars show the runtime of a function with a random instruction schedule.
    The two runs of the instruction schedule are grouped together.
    The distance measure represents the longest execution time relative to the shortest execution time.
    Two runs that differ more than 5\% are marked as outliers.
    However, this processor did not produce any outliers in our experiment.}
    \label{fig:eval:rndm:aurora}
\end{figure}

Multiple reasons could lead to the small or missing variations that we observe in \Crefrange{fig:eval:rndm:aarch64:e}{fig:eval:rndm:aarch64:f} and \Crefrange{fig:eval:rndm:aurora:e}{fig:eval:rndm:aurora:f}:
One possibility is that different instruction schedules have no effect on the runtime of the basic block.
The motivation for this experiment was to verify, that these cases do not dominate all the instruction schedules.
Another possibilities have their origin in the experimental setup, these could be:
\begin{itemize}
    \item The basic block for which we manipulate the instruction schedule might have a low influence on the runtime of the function.
        We tried to minimize this effect by choosing basic blocks that are often executed.
    \item Our random instruction scheduler works on top of LLVM.
        LLVM makes, in this stage of the back-end, still use of pseudo-instructions that are not represented in the binary.
        This means that schedules that we see as different schedules might actually end up being equal in the binary.
    \item There are short functions with a short execution time.
        We observed few changes in the runtime when the measured execution time is below 10,000 processor cycles.
        The underlying timer of the C++ standard library might not be able to measure such short time periods.  
\end{itemize}
However, the experiment is still valid, because we show that we are able to influence the runtime by manipulating the instruction schedules beyond noise in the measurements.

In summary, we observe different runtimes for different instruction schedules and the results are reproducible over multiple runs.
This is not true for all basic blocks, but the goal of this experiment was to show the existence of an effect of the instruction schedule on the runtime.
These results motivate further research with two goals:
First, to automatically optimizing instruction scheduling for basic blocks.
Secondly, to do that only for basic blocks that influence the runtime a lot in order to speed up compilation times.

\section{Learning to Schedule}
\label{sec:approach:ml}
Our goal is to create a model that can generate good instruction schedules for basic blocks.
Therefore, we have to define a mapping that we want to learn with this model.
The possible inputs can be complex, like characteristics of the hardware, the register states, or others.
However, we decided for this thesis to begin with a simple model and limit our input to the $n$ last scheduled instructions and the set of instructions that can be scheduled.
These are important because they might utilize different execution units on the processor, and thus generate pipeline effects.

We define this mapping as
\begin{equation}
    (h_n, \ldots, h_1) \times \{i_1, \ldots, i_m\} \mapsto i_k
    \label{eqn:mlmapping1}
\end{equation}
where $h$ defines the instructions in the history of scheduled instructions, $i$ are instructions that can be scheduled and $k \in \{1, \ldots, m\}$.
However, we define a second, easier to implement, mapping
\begin{equation}
    (h_n, \ldots, h_1) \times i \mapsto r
    \label{eqn:mlmapping2}
\end{equation}
where we map from the history and a single instruction to a reward value $r$.
We lose some input information in this model about the other available instructions.
But we can now simply run the model for all the available instructions and select the one with the best reward.

We need lots of data samples to utilize data-driven, or machine learning approaches.
One way would be to generate valid random schedules.
The downside of this approach is that we will generate many bad schedules because the better ones are rare.
A better way is to use \ac{mcts} to iteratively find better schedules (see \Cref{subsec:approach:ml:mcts}).
The \ac{mcts} approach searches for good schedules locally, \ie for a single basic block.
So, the search is not generalized for other basic blocks.

To generate a model that works globally, \ie also for unseen basic blocks, we have to add a second step and learn from the locally good schedules.
Therefore, we extract data samples matching the input format of \Cref{eqn:mlmapping1}, or respectively \Cref{eqn:mlmapping2}.
We have trained different models that are explained in detail in \Cref{subsec:approach:ml:global}.

\subsection{Local MCTS Model}
\label{subsec:approach:ml:mcts}
We choose \ac{mcts} to find examples of schedules that perform best.
The reason is that this approach fits naturally for sequential problems and allows us to balance between the exploration and exploitation of instruction schedules. 
\ac{mcts} tries all possible next steps in a given situation and chooses the further steps randomly because there is no information available.
When all possibilities in a situation are already examined, the algorithm takes the best option (based on a defined metric) and tries to improve from thereon.
It does not choose the best option every time but also incorporates exploration choices.
This enables us to search for good instruction schedules.
See \Cref{sec:bg:mcts} for more details.

% Explain structure of the tree: nodes = instructions, edges = valid traversion of dag
The input to our \ac{mcts} approach is a \ac{dag} generated by the LLVM compiler, which defines the valid instruction schedules.
See \Cref{fig:bg:llvm-dag} for an example.
The LLVM instruction \ac{dag} still contains many instructions that exist just for internal reasons and are not translated to machine instructions.
We remove these pseudo-instructions from the instruction \ac{dag} and adjust the dependencies between the nodes.

The nodes in an \ac{mcts} tree represent a state in the underlying model, and the edges correspond to transitions between these states.
Compare \Cref{fig:approach:max-vs-avg:a} to see that our \ac{mcts} nodes represent a (partial) instruction schedule.
The edges in our \ac{mcts} tree represent a legal scheduling of a new instruction to the previous schedule.
When you iterate back from any node to the root node, you get the (partial) instruction schedule in reverse ordering.
\begin{figure}[p]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=17.5cm,angle=90]{data/mcts-max-vs-avg/svg/selected/8752639012199.pdf}
        \caption{\ac{mcts} tree}
        \label{fig:approach:max-vs-avg:a}
    \end{subfigure}
    \hspace{0.2\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=17.5cm,angle=90]{data/mcts-max-vs-avg/svg/selected/8752639012199-next.pdf}
        \caption{\ac{mcts} tree after next iteration}
        \label{fig:approach:max-vs-avg:b}
    \end{subfigure}
    \caption[\ac{mcts} tree with the consequences of maximum and average metric]{
        \Cref{fig:approach:max-vs-avg:a} shows a given \ac{mcts} tree and \Cref{fig:approach:max-vs-avg:b} shows two different choices on different \ac{mcts} metrics.
        The boxes represent a \ac{mcts} node, at the top is the name of the instruction, on the left are the cumuluated metric and the number of visits, on the right is the metric.
        In this example, we use the negative runtime for the metric.
        The dashed box is the new instruction.
        The green values show the alternative maximum metric values.
        The red path was chosen if we used the average metric (-122.0), but if we use the maximum metric (green path) we find a better runtime (-102.0).
        % \\ (Benchmark: LoopRestructuring-dbl, Function: s2233, Basic Block: for.body20)
    }
    \label{fig:approach:max-vs-avg}
\end{figure}

We do not use the default choice of node evaluation metric.
Typically, to evaluate the reward of a node in the \ac{mcts} tree, you compute the average of the children nodes.
In our approach, this leads to situations where we have already found a good schedule but keep searching for schedules in another area of the tree.
The reason for that is that the good schedule might be surrounded by relatively bad schedules, which have a negative effect on the average evaluation.
When you train \ac{mcts} for games, that makes sense because you do not want to get into situations where only one good state exists, and the surrounding ones would let you lose the game.
But we are only interested in a single best schedule, therefore we do not care about similar but bad schedules.
The consequence is that we do not take the average but the maximum.
This means that in the exploitation phase, we always follow the path to the best-found schedule.
You can observe the consequences in \Cref{fig:approach:max-vs-avg:b}, where the green numbers represent the maximum metric and the black numbers represent the average metric.
Our approach is similar to the one in~\cite{bjornsson2009cadiaplayer}, however, they still use the average for the search part.

% Explain process: Generate schedule -> compile -> measure runtime -> train mcts -> go to 1
The \ac{mcts} learning process is separated into four phases:
\begin{enumerate}
    \item Generate schedule: \\
    Apply the exploration or exploitation phase and generate a new schedule based on our previous experience.
    This results in most situations in a partial instruction schedule because the \ac{mcts} tree usually not grows until it knows all states.
    The partial instruction schedules are completed by using random scheduling decisions. 
    \item Compile the generated schedule: \\
    The generated schedule is transformed into a C++ file with a function that contains the inline assembly of the scheduled basic block.
    We then compile the schedule with our benchmarking framework (see \Cref{sec:approach:isol_bb_exec}).
    \item Execute and measure the runtime: \\
    The compiled executable is transferred to our target machine and executed there to measure the runtime of the basic block (see \Cref{sec:approach:isol_bb_exec}).
    \item Progress the \ac{mcts} tree with the measured runtime: \\
    Then, the measurement data is copied back to the machine where we run the experiment.
    The measurement data is preprocessed and used to train the \ac{mcts} tree.
\end{enumerate}
This process is repeated many times, depending on the experiment.
See \Cref{sec:bg:mcts} for more information.

% Technically possible to use for production, but slow
Note that this \ac{mcts} approach is not only able to generate data for our global model.
It can theoretically also be used in a normal compilation process.
It is not practical to use it for a whole program because of the many compilation steps and the bad compilation-time performance.
However, critical parts of a program could be optimized with this approach.

% Positive sideeffect: Also there to define a possible range of improvement
A positive side effect of this \ac{mcts} approach is that we generate an upper limit for the improvements of the instruction scheduling phase.
The further we grow the \ac{mcts} tree, the higher is the probability that we find the actual upper limit of improvements.
This is at the same time an upper limit for the global models in our second step.
Therefore, we can see how much of our potential improvements we are able to reach with our global models.

\subsection{Global Model}
\label{subsec:approach:ml:global}
Once we have found good schedules by generating \ac{mcts} trees for many basic blocks, we want to generalize this knowledge.
This is necessary to transfer the knowledge to unseen basic blocks and make the model usable in practice by generating good schedules directly, in contrast to searching them with \ac{mcts}.
Therefore, we have developed three different models that we discuss here.

Another way to see this two-step approach is to see the \ac{mcts} step as a training phase and this global model step as inference phase.

\subsubsection{Nearest Neighbor Model}
\label{sec:app:nearest-neighbor}
Our first model is a nearest neighbor approach.
That means we iterate our \ac{mcts} trees and build up a big model in which we can search for situations and see what decision performed best.

% Dataformat: (h_n, ..., h_1) x (i_1, i_2) -> (w_1, d, w_2)
In order to build the model, we first define the input and output formats.
A scheduling situation has the format
\begin{equation}
    (h_n, \ldots, h_1) \times {i_1, \ldots, i_m}
    \label{eqn:approach:example-scheduling-situation}
\end{equation}
where instructions $h$ build a sequence of last scheduled instructions and instructions $i$ build the set of available instructions for scheduling.
To simplify our model and have more data per scheduling situation, we limited the size of the set of available instructions to two.
The disadvantage is that we lose some context information on which other instructions are available.
The data we are interested in per scheduling situation is the number of times instruction $x$ was better than instruction $y$ or vice versa and the number of times they were on par.
This results in the data format
\begin{equation}
    (h_n, \ldots, h_1) \times (i_1, i_2) \mapsto (w_1, d, w_2)
\end{equation}
where $w_1$ and $w_2$ represent how often instruction $i_1$, or $i_2$ was the better choice.
How often they were on par is counted by the variable $d$.

% How to build this model?
To build the model, we iterate the \ac{mcts} trees and extract the contained data.
\begin{figure}
    \centering
    \begin{forest}
        [$h_n$
            [$\vdots$
                [$h_1$
                    [$i_1$] [$i_2 $] [$i_3$]
                ]
            ]
        ]
    \end{forest}
    \caption[Example scheduling situation]{Example scheduling situation with the previously scheduled instructions $h$ and the instructions $i$ to choose from.}
    \label{fig:approach:example-scheduling-situation}
\end{figure}
For every scheduling situation (compare \Cref{eqn:approach:example-scheduling-situation} and \Cref{fig:approach:example-scheduling-situation}), we take every combination of instructions $i$.
For each combination, we check which instruction choice resulted in a better reward.
We add this situation to our model if it was not already present.
Otherwise, we increase the corresponding counter variable in this situation.

In addition to this situation, we can also learn from this situation for a scheduling situation with a shorter history.
So we also add the situation for all partial histories.
For example, when we have a scheduling situation with the history $(h_3, h_2, h_1)$, we also add our learnings to the model with the histories $(h_2, h_1)$, $(h_1)$, and the empty history $()$.

% How to make scheduling decisions from this model?
This model can be easily applied for scheduling.
In a given scheduling situation, we just search for the history and all combinations of available instructions.
If we have found all the combinations, we can make our decision based on the counts.
For the situation where we did not find all the required combinations, we define a percentage threshold $t_c$.
If we found more than $t_c$ instruction combinations, we make a choice based on the instructions that we have found.
If we found fewer than $t_c$ instructions, we remove the oldest instruction $h_n$ from the history and search again for all the instruction combinations. 

\subsubsection{Support Vector Machine Model}
% What mapping are we trying to learn?
Further, we want to learn parametric regression models.
Therefore, we need a different mapping that we learn because our output will be a scalar value.
\begin{equation}
    (h_n, \ldots, h_1) \times i \times (d_{min}, d_{max}) \mapsto r
    \label{eqn:approach:regression-mapping}
\end{equation}
We choose to have a sequence of history instructions $(h_n, \ldots, h_1)$ with a defined length $n$.
From the history and the candidate instruction $i$ we map to the reward value extracted from the \ac{mcts} data.

One additional metric that we add to our input is the distance into the history of the direct \ac{dag} predecessor instructions.
We do this because, instructions have to wait for the result of the direct predecessor instructions when they have a data dependency.
And the bigger the distance between them is, the higher is the probability, that the predecessors already terminated. 
For example, instruction $a$ depends on instruction $b$, and we are in the situation in which we want to schedule $a$.
That means we have already scheduled $b$.
There usually also exist other instructions that we scheduled after $b$.
We count the number of instructions that we scheduled between $a$ and $b$.
There is typically more than one instruction that $a$ depends on, so we decided to take the maximum and the minimum number of instructions between $a$ and its predecessors for the dataset.
We refer to these values as $d_{max}$ and $d_{min}$ in \Cref{eqn:approach:regression-mapping}.

% How was the input data generated?
To build a dataset with the mapping defined in \Cref{eqn:approach:regression-mapping} we iterate the \ac{mcts} trees.
For every instruction node, we collect its previously scheduled instructions and its reward value.

% Data preprocessing
However, we cannot insert these data points into the \ac{svm} model because it requires numerical values only.
We transform the instructions from their textual representation into an ordinal encoding, \ie every instruction gets a unique numerical representation.
Further, we scale all numerical values into the range $[0,1]$ because \acp{svm} typically learn better when inputs are in this range.
\todo{Add table?}

With this data, we train an \ac{svm} regression model.
The details and results of the experiments can be found in \Cref{sec:eval:svm}.

\subsubsection{Neural Network Model}
We choose to use a neural network as a second regression model, because they have a shorter inference runtime when trained on big datasets than \acp{svm}.
With this model, we try to learn the same mapping as we did with our \ac{svr} model, which is defined in \Cref{eqn:approach:regression-mapping}.
Consequently, we use the same dataset that we have already built for the \ac{svr} model.

In contrast to our \ac{svr} model, we chose not to encode the instructions into ordinary values.
The instructions are encoded into one-hot vectors because neural networks can handle high-dimensional inputs better.
Further, the we also scale numerical values into the range $[0,1]$, like we did for the \ac{svr} model.

We use a standard neural network with three hidden layers with dimensions 512, 128, and 64.
The output layer is a singular value which represents the scaled score of the given scheduling situation.
All hidden layers are followed by a ReLU activation function~\cite{nair2010rectified}.
We use the Adam optimizer~\cite{kingma2014adam} with a learning rate of $10^{-3}$.
For the loss function, we use the squared error.

We discuss the details and results in \Cref{sec:eval:nn}.

\section{Data Generation}
We cannot build our data-driven models (see \Cref{sec:approach:ml}) directly from our dataset, described in \Cref{sec:approach:dataset}, as it requires some transformation from the code to a metric we are interested in.
Many metrics can be chosen for optimization, \eg runtime, energy consumption, cache misses, processor stalls.
We focus on optimizing the instruction scheduling for short runtimes.
To generate data for our MCTS approach, we need a mapping from instruction schedules to runtimes.

The most reliable approach is to measure the runtime while executing the code.
However, research on approaches that rely on analytical models exists, see \Cref{sec:rw:other:runtime} %~\cite{llvm:mca, mendis2019ithemal, taha2003instruction, laukemann2018automated}.
We will choose the classical approach of runtime measurements since we have access to the hardware, and the resulting data will be more accurate.
Therefore, we compile the code with varying instruction schedules and execute it to measure its runtime.
We discuss various aspects of this in the remainder of this section.

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{img/ppt/runtime_measurement_scopes-crop.pdf}
    \caption[Possibile scopes for measuring runtimes]{The outer box represents a function. 
    The function consists of multiple basic blocks (BB) where different possible branches are caused by if/else statements and loops. 
    The diagram shows three possible runtime measuring scopes to measure the runtime of the filled basic block.
    Method 1 shows a measuring inside of a basic block. 
    With method 2, the measurement starts with the end of the last basic block and ends with the start of the next one.
    Method 3 measures the complete function which surrounds the basic block.}
    \label{fig:approach:runtime_scopes}
\end{figure}

\subsection{Runtime Measurement Unit}
\label{sec:approach:runtime-measurement-unit}
The metric we are interested in specifically is the relative runtime compared to a baseline runtime.
Runtime overheads can be ignored in our approach, as long as they are constant over multiple runs and the baseline has the same overheads.
Thus, various possibilities for placing the start and stop events of the timer exist to accomplish runtime measurements of fractions of the application's code.
The objective is to minimize noise and get reliable, reproducible measurements.
We discuss the possibilities of making measurements on a basic block, function, and application scope and relevant aspects to these possibilities.

\subsubsection{Application}
\label{sec:approach:datageneration:runtime:app}
The most straightforward approach to measuring the runtime of a piece of code is to measure the runtime of its whole application.
The disadvantage is that we include many more instructions in the measurement than we are interested in.
This generates lots of noise from various possible sources (\eg from \ac{io} instructions like opening files, printing to the screen, or varying latencies for memory accesses).
Also, the longer the runtime of a program, the higher is the probability that the operating system interrupts the execution for more important processes.
Depending on the used measurement method (see \Cref{sec:approach:datageneration:runtime_methods}), this may be included or not be included in the measurement.
These disadvantages make this approach unreliable and not usable for us because we are interested in the short runtime of a single basic block.

\subsubsection{Function}
\label{sec:approach:datageneration:runtime:function}
A finer-grained approach is to start the timer at the beginning of the function or method that contains the basic block of interest and stop the timer at the end of the function (see Method 3 in \Cref{fig:approach:runtime_scopes}).
This approach creates several problems.

Compared to measuring the runtime of the whole application (see \Cref{sec:approach:datageneration:runtime:app}), this approach executes only a small fraction of the code.
This results in a lower degree of variance in the measured runtimes due to fewer \ac{os} interruptions, and fewer memory and \ac{io} accesses.
Thus, the precision of these measurements is higher.

Notice in \Cref{fig:approach:runtime_scopes} that different control paths are possible in a function, based on if/else statements or loops.
This can be problematic in a general setup.
However, the benchmarks we use and the data we insert to them are deterministic, \ie the same paths are taken in each benchmark execution.
Theoretically, if there was no variance in the measurements, the shortest measured execution time will always correspond to the same execution path.
In practice, we get many varying measurements because of different execution paths.
That makes it difficult to differentiate between execution paths and variance in the measurements.

\subsubsection{Basic Block}
The most intuitive approach when scheduling basic blocks is to measure only the runtime of the basic block we are interested in.
We eliminate all extra processor instructions that we are not interested in.
However, we can still get runtime variations for different reasons:
\begin{itemize}
    \item The basic block could call other functions which could, in turn, execute varying branches. 
          See \Cref{sec:approach:bbisolation} for a discussion about this.
    \item The \ac{os} could interrupt the execution. 
          But the probability decreases with shorter code fragments.
\end{itemize}

One requirement that can not be met in all situations is that of a very precise timer.
These timers usually depend on the hardware directly, \ie the hardware must provide such functionality to measure runtimes.
This is possible on our chosen hardware and is discussed in more detail in \Cref{sec:approach:hwpercounter}.

Thus, as this is a feasible solution for our experiments, we have chosen to measure the runtime of basic blocks alone.
    
\subsection{Runtime Measurement Methods}
\label{sec:approach:datageneration:runtime_methods}
% \todo{Could add little experiment where we compare hardware timers vs OS access}
Various possibilities to observe execution times exist in systems that we use for our research.
These can typically be classified into the categories of profiling, \ac{os} utilities, or hardware utilities.
We discuss these three in more detail.

\subsubsection{Profiling}
Profilers are programs that observe the execution of other processes running on the system (\eg gperf \cite{graham1982gprof}).
They are designed to observe the execution from start to end and to return various metrics. Some examples are:
\begin{itemize}
    \item execution times per function
    \item memory consumption
    \item cache misses
\end{itemize}
The observation is done by doing snapshots of the process of interest.
Thus, profilers are good for finding, \eg performance problems and memory leaks in software.

However, we need cycle-accurate measurements from instruction A to instruction B for our experiments.
Profilers are not able to make such precise measurements, \ie they are not helpful to us because they are designed for other purposes.

\subsubsection{Operating System Methods}
\acp{os} typically need to work with times and also provide a user interface to use its timing functions.
There is the wall clock time and timers that are based on \ac{cpu} functionality to get times.
We are interested in the latter.
Often there are various possibilities to retrieve timing values from the hardware with varying precision.
The most precise timers are usually good for measuring intervals in the microseconds range or longer.
\ac{os} functions have some overhead because they also handle errors.
\begin{table}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        Operating System & Timer in C language \\
        \midrule
        Linux & \lstinline|clock_gettime()| \\
        Windows & \lstinline|QueryPerformanceCounter()| \\
        \bottomrule
    \end{tabular}
    \caption[Operating System interfaces to their timing functions]
    {
        The table shows examples for interfaces in \acp{os} to their timing functions in the C programming language.
    }
    \label{tab:approach:timing_functions}
\end{table}
See \Cref{tab:approach:timing_functions} for selected \acp{os}.
The C++ function \lstinline|std::high_resolution_clock()| from the standard library falls back to one of these \ac{os} functions.

\acp{os} sometimes not only provide timing methods but also allows us to manage how a process will be executed.
Linux provides different methods to avoid or reduce the probability of \ac{os} interrupts during the execution of a process.
\begin{itemize}
    \item Isolating \ac{cpu} cores: 
    The Linux kernel command line option \lstinline|isolcpus=[cpu_number]| in the boot loader instructs Linux to not schedule tasks to the given \ac{cpu} cores automatically.
    \item Pinning a process to a \ac{cpu} core: 
    The Linux program \lstinline|taskset| can specify on which \ac{cpu} core a process will be scheduled.
    This can also be used to schedule processes on isolated \ac{cpu} cores.
\end{itemize}
When we pin a process to an isolated \ac{cpu} core, this task will not be interrupted by the \ac{os}.
This makes the measuring of runtimes more reliable.

\subsubsection{Hardware Performance Counters}
\label{sec:approach:hwpercounter}
The most code blocks we want to measure are only 5-50 instructions long, \ie we need very precise measurements.
We use the assembly instructions of the given hardware directly to get the most precise timing values with the lowest overhead.
These instructions typically return the value of a processor cycle counter register.

Using assembly instructions can be complicated (depending on the used high-level language and hardware).
But it is not a problem in our experiments because the code blocks we want to measure are already written in the assembly language of the given hardware.
That means we only add a timer instruction at the beginning and one at the end.
We do some error checking after the execution has stopped (see \Cref{sec:approach:isol_bb_exec} for details).
\begin{table}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        Hardware & Assembly instruction \\
        \midrule
        x86 & \lstinline|rdtsc| \\
        ARM32 & \lstinline|mrc p15m, 0, r0, c15, c12, 1| \\
        AArch64 & \lstinline|mrs x0, PMCCNTR_EL0| \\
        \aurora & \lstinline|smir %s60, %usrcc| \\
        \bottomrule
    \end{tabular}
    \caption{Assembly instructions for getting the cycle counter value of a given hardware.}
    \label{tab:approach:asm_instrs_timing}
\end{table}
See \Cref{tab:approach:asm_instrs_timing} for the assembly instructions per hardware.

Depending on the hardware and the \ac{os}, access to these instructions or registers might not be permitted.
For the AArch64 architecture and a Linux \ac{os}, we had to implement and load a kernel module that granted access to this register.
 

\subsection{Execution Unit}
% \todo{Provide runtime example data}
\label{sec:approach:exec_unit}
For the learning process, we have to execute the differently scheduled basic blocks many times.
If we executed the whole benchmark that contains the basic block of interest, it would be time-intensive, and many lines of code were executed that we are not interested in.
This forces us to extract the basic block from the benchmark and execute it in isolation.
Here, we discuss the advantages and disadvantages of the two possible approaches.

\subsubsection{Benchmark}
The first option is to execute the complete benchmark for every new schedule that was generated during the learning process.
The advantage is that we execute the original basic block as it would appear in the real benchmark.
The problem with this approach is that the execution time of a benchmark can be high and makes the execution the bottleneck in our pipeline.
Many benchmarks in our dataset take several minutes to execute on a RaspberryPi 3B.
So, always executing the whole benchmark is costly to even get only one runtime measurement per schedule.
In order to get statistically relevant runtimes, we have to measure them multiple times.

\subsubsection{Basic Block}
The alternative to executing the whole benchmark is to extract the scheduled basic block from the benchmark and execute it in isolation.
By using this approach, we reduce the execution time to a minimum and do not execute code we are not interested in.
Consequently, we can execute the benchmark many times and get statistically reliable runtime measurements.
The disadvantage is that we have to slightly modify the basic block or the program will crash.

\subsection{Basic Block Isolation}
\label{sec:approach:bbisolation}
As discussed in \Cref{sec:approach:exec_unit}, we are forced to execute basic blocks isolated from the benchmark.
To make a basic block executable, we isolate it and insert it as inline assembly into our minimal C++ framework.

\subsubsection{Basic Block Extraction}
% \todo{include numbers on speedup vs. benchmarking full function}
First, we compile the benchmark and schedule the basic block with our modified compilation process (see \Cref{sec:approach:build_process}) into assembly files.
Next, we extract the basic block instructions from the assembly file.
Then, we modify the basic block instructions to prevent execution and compilation errors, and as the last step, we inject it into our C++ framework.
We describe the modifications in the next paragraphs.

Assembly files contain comments, labels, and assembler directives which we remove.
Comments are useless lines for the execution task. 
The labels are not required, as we do not jump to other instructions.
Assembler directives represent commands to the assembler program for bookkeeping, storage reservation, and other purposes.
These could interfere with our compilation process and are also removed.

Basic blocks can contain jump instructions as the last instruction and function calls in any position.
We only schedule the instructions in the given basic block, so we are also only interested in its own runtime.
Executing other code, which could internally choose different paths, would add noise to our measurements.
To prevent this uncontrolled program execution, we simply remove jump and call instructions.
The removal of these instructions might slightly reduce the execution time, but the reduction is constant over all executions with different schedules.

In order to prevent segmentation faults, we have to redirect all memory accesses to a memory area that we are allowed to read and write to.
This affects all load and store instructions.
In our framework, we allocate 8 byte on the stack where we redirect all memory accesses.
This also means that all memory accesses are cache hits, and we have a controlled environment in this aspect.

\Cref{fig:approach:bb-cleaning} shows an example in which we manipulate a basic block.
We remove a label and comments, redirect memory accesses and remove a jump instruction.
\begin{figure}
    \begin{subfigure}{0.65\textwidth}
        \begin{lstlisting}
.LBB9_14:               // %for.body82
                        //   Parent Loop BB9_2 [..]
                        //     Parent Loop BB9_[..]
                        // =>    This Inner Loo[..]
sub	x24, x23, #8        // =8
cmn	x24, #8             // =8
ldr	d0, [x21]
ldr	d1, [x19, x23]
ldr	d2, [x22, #-8]!
ldr	d3, [x7, x23]
fmul	d2, d2, d3
fsub	d0, d0, d2
fdiv	d0, d0, d1
str	d0, [x21]
mov	x21, x22
mov	x23, x24
b.ne	.LBB9_14
        \end{lstlisting}
        \caption{Original assembly}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \begin{lstlisting}[showlines=true]




sub	x24, x23, #8
cmn	x24, #8
ldr	d0, [%2]
ldr	d1, [%2]
ldr	d2, [%2]
ldr	d3, [%2]
fmul	d2, d2, d3
fsub	d0, d0, d2
fdiv	d0, d0, d1
str	d0, [%2]
mov	x21, x22
mov	x23, x24

        \end{lstlisting}
        \caption{Manipulated assembly}
    \end{subfigure}
    \caption[Assembly Basic Block Manipulation]{AArch64 Assembly Basic Block Manipulation:
    We remove the first line because it is a label.
    We remove the comments in the lines two to 6.
    Then, we replace the memory accesses with the placeholder \lstinline|\%2|, which will be replaced by the compiler later (see \Cref{fig:approach:bb-executable}).
    Lastly, we remove the jump instruction in line 17.}
    \label{fig:approach:bb-cleaning}
\end{figure}

\subsubsection{Isolated Basic Block Execution}
\label{sec:approach:isol_bb_exec}
To execute the extracted basic block, we have to surround it with some other code to generate a valid C/C++ program.
For this, we have written a small benchmarking framework.
\Cref{fig:approach:bb-executable} shows an example of the basic block from \Cref{fig:approach:bb-cleaning} integrated into this framework.
We also add the \ac{cpu} timer calls and mark the used registers as clobbered.
% This framework is compiled together with the given basic block in the inline assembly format.
The optimization is turned off in this compilation step, so the basic blocks are executed as given.
\begin{figure}
    \begin{lstlisting}[
        % language=C++
    ]
#pragma once

#include <cstdint>

struct Benchmark {
public:
    __attribute__((always_inline)) uint64_t operator() () {
        uint64_t a[4] = { 0xcdab, 0xcdab, 0xcdab, 0xcdab };
        uint64_t* p = a;
        uint64_t start, end;
        asm volatile(
            "mrs        %0, PMCCNTR_EL0 \n\t"
            "sub	x24, x23, #8	\n\t"
            "cmn	x24, #8         \n\t"
            "ldr	d0, [%2]	\n\t"
            "ldr	d1, [%2]	\n\t"
            "ldr	d2, [%2]	\n\t"
            "ldr	d3, [%2]	\n\t"
            "fmul	d2, d2, d3	\n\t"
            "fsub	d0, d0, d2	\n\t"
            "fdiv	d0, d0, d1	\n\t"
            "str	d0, [%2]	\n\t"
            "mov	x21, x22	\n\t"
            "mov	x23, x24	\n\t"
            "mrs        %1, PMCCNTR_EL0 \n\t"
            :  "=r"(start), "=r"(end)
            :  "r"(p)
            :  "q0", "q1", "q2", "q3", "x21", "x23", "x24",  "memory", "cc"
        );
        return end > start ? end - start : 0;
    }
};        
    \end{lstlisting}
    \caption[Assembly Basic Block Integration into Execution Framework]{AArch64 Assembly Basic Block Integration into Execution Framework:
    This example shows the basic block from \Cref{fig:approach:bb-cleaning} how it is integrated into the C++ framework.
    The compiler replaces the placeholder \lstinline|\%2| with a pointer to the array \lstinline|a| (line 8).
    We also list the used registers, so the compiler knows that their values have changed (line 28).
    The first and the last instructions in the inline assembly (lines 12, 25) call the \ac{cpu} timer.}
    \label{fig:approach:bb-executable}
\end{figure}

The executable first executes the basic block in 100 warmup rounds to prevent side effects from caches and other reasons.
The actual execution where we measure the runtime is repeated until 1,000 valid measurements are done.
Invalid measurements can occur when the runtime is negative.
This happens when the cycle counter overflows.

\subsection{Computing Rewards From Runtimes}
Even though we do the warmup rounds, we still get few data points that are clearly outliers and should not be included in our experiments.
We sort the 1,000 runtime measurements and strip the top and bottom 50 measurements.
Spot checks have shown that we remove all the outliers in almost all the experiments.
\begin{table}
    \centering
    \begin{tabular}{@{}rr@{}}
        \toprule
        Runtime & Num. Occurances \\
        \midrule
        140 & 998 \\
        143 & 1 \\
        711 & 1 \\
        \bottomrule
    \end{tabular}
    \hspace{0.8cm}
    \begin{tabular}{@{}rr@{}}
        \toprule
        Runtime & Num. Occurances \\
        \midrule
        121 & 708 \\
        120 & 289 \\
        123 & 1 \\
        163 & 1 \\
        1491 & 1 \\
        \bottomrule
    \end{tabular}
    \hspace{0.8cm}
    \begin{tabular}{@{}rr@{}}
        \toprule
        Runtime & Num. Occurances \\
        \midrule
        114 & 994 \\
        121 & 2 \\
        115 & 1 \\
        129 & 1 \\
        130 & 1 \\
        694 & 1 \\
        \bottomrule
    \end{tabular}
    \caption[Example runtime measurement distribution]{Example runtime measurement distribution for three different schedules. (Benchmark: almabench, Function: anpm, Basic Block: entry)}
    \label{tab:approach:example_runtimes}
\end{table}
See \Cref{tab:approach:example_runtimes} for an example with three different schedules for the same experiment.

Because different basic blocks have their different runtimes, the absolute measured speedups between two basic blocks are not comparable.
To have a metric $m$ that we can compare with all other basic blocks in the data set, we normalize the experiment runtime $r_e$ with the runtime of the baseline scheduler $r_b$.
\begin{equation}
    m=\frac{r_b}{r_e}
    \label{eqn:approach:mcts-score}
\end{equation}

% \section{Application of the Learned Model}
% \label{sec:approach:ml-scheduler}
% \todo{Write about how we run the learned models and how we make measurements}
