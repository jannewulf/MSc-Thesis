\chapter{Approach}
\todo{Why are we using a data-driven / machine learning approach? Show that it is complicated to do it by hand. Also write why not to do auto-tuning.}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/ppt/approach_overview-crop.pdf}
    \caption[Overview of the approach]{Overview of the overall approach. 
    The process covers the selection of basic blocks, the instruction schedule generation, the execution model, the learning process and the derivation of the final scheduler.}
    \label{fig:approach:overview}
\end{figure}
In the rest of this chapter, we will present the approach used for this thesis (compare~\cref{fig:approach:overview}).
We start with an overview of the benchmark programs we used~(\cref{sec:approach:dataset}).
Then we discuss the importance of basic blocks for instruction scheduling in general and our approach specifically~(\cref{sec:approach:basicblock}).
That is followed by explaining the \ac{mcts} approach~(\cref{subsec:approach:ml:mcts}).
Eventually, we introduce how we combine individual \ac{mcts} models into a global model~(\cref{subsec:approach:ml:global}) and derive an applicable scheduler~(\cref{sec:approach:ml-scheduler}).

\section{Dataset}
\label{sec:approach:dataset}
For our experiments, we use benchmarks from the LLVM project called LLVM Test Suite~\footnote{\url{https://llvm.org/docs/TestSuiteGuide.html}}.
The test suite contains benchmarks in the following categories
\begin{itemize}
    \item Single-Source --- built from a single C/C++ source file
    \item Multi-Source --- built from multiple C/C++ source files
    \item Micro-Benchmarks --- seperate functions, that are executed by google-benchmark
    \item Bitcode --- tests that are written in LLVM bitcode
    \item CTMark --- symbolic links to the other benchmarks to measure compilation performance
\end{itemize}
We select the 273 benchmarks from the Single-Source and Multi-Source categories for our experiments.
\todo{Add table?}
The benchmarks are all written in C or C++.
These categories contain benchmarks that are compiled from a single source file or multiple source files, respectively.
In addition, the test suite contains more benchmarks that are not relevant to our experiments or took too long to integrate.

\subsection{Modified build process}
The test suite comes with Makefile's and CMake configuration to automatically build all the benchmarks.
However, these are not sufficient for our approach.
We need a modified build process for all the benchmarks because we are manipulating it in several cases.
Furthermore, the compilation cannot be done with a single command because not all arguments of the separate compilation steps are available in the main compiler.
We execute the front-end, optimization, and back-end phase separately to have complete control over the build process.

We have to set some compiler arguments to make the LLVM compiler front-end eject LLVM \ac{ir} instead of compiling the program completely.
For C programs we use \lstinline{clang}~\footnote{\url{https://clang.llvm.org/}} and for C++ programs we use \lstinline{clang++}.
However, the compiler arguments are the same.
We pass the following arguments to the front-end:
\begin{itemize}
    \item \lstinline{-O0}: to prevent optimization in this first step
    \item \lstinline{-Xclang -disable-O0-optnone}: the \lstinline{-O0} flag alone also prevents optimization in further steps, which is not what we want
    \item \lstinline{-S -emit-llvm}: to emit LLVM \ac{ir} for further processing --- instead of compiling the program completely
\end{itemize}

The LLVM optimizer \lstinline{opt}~\footnote{\url{https://llvm.org/docs/CommandGuide/opt.html}} gives a choice to select one of the predefined optimization levels or select specific optimizer runs.
When we compile a benchmark for measuring the runtime, we only use the flag \lstinline{-O3} for optimizing the LLVM \ac{ir}.
The LLVM framework makes it easy to implement new optimizer passes.
We do this to count how often a basic block is executed (see \cref{sec:approach:basicblock:selection}) and measure a function's execution time (see \cref{sec:approach:datageneration:runtime:function}).

We manipulate the instruction scheduling by adjusting how we call the compiler back-end.
The LLVM static compiler implements two instruction schedulers before the register allocation and one after the register allocation.
The former inherit either from the C++ class \lstinline{SelectionDAG} or from \lstinline{MIScheduler}.
Which one will be executed by the compiler depends on the target architecture. 
It is also possible that the compiler executes both.

We manipulate the back-end by passing arguments to the LLVM static compiler \lstinline{llc}~\footnote{\url{https://llvm.org/docs/CommandGuide/llc.html}}.
This program represents the back-end phase and executes the instruction scheduling.
We use the following arguments for the back-end:
\begin{itemize}
    \item \lstinline{--pre-RA-sched=}: to select our new \lstinline{SelectionDAG} schedulers for the pre-register-allocation phase
    \item \lstinline{--misched-cutoff=0}: to disable the \lstinline{MIScheduler}
    \item \lstinline{--disable-post-ra}: to disable the post-register-allocation scheduler
\end{itemize}

We analyze the CMake configurations and extract required compiler arguments on a per benchmark level.
We categorize the extracted arguments into front-end, optimizer, and back-end phases.
Finally, we append them to the previously described arguments in the respective compilation phase.

We also analyze the test suite for execution arguments.
For example, some benchmarks read data from files or require arguments when calling them.
To execute them, we generate a shell script that calls the compiled benchmark with the required arguments.

\section{Basic Block --- scheduling unit}
\label{sec:approach:basicblock}
\begin{itemize}
    \item What: We choose basic blocks as the scheduling unit
    \item Why: Used by the LLVM compiler framework and most other compilers, Limits scope and complexity
    \item How: Code split into BB's and scheduling of BB's independently
\end{itemize}

\subsection{Selection process}
\label{sec:approach:basicblock:selection}
\begin{itemize}
    \item What: Select BB's for our dataset
    \item Why: Too many to work on all of them, time intensive, compilation time, execution time, ML
    \item How: Develop heuristics for the selection
\end{itemize}
\subsubsection{Longest basic blocks}
\begin{itemize}
    \item What: The BB's with the most instructions in them
    \item Why: Good heuristic for number of scheduling decisions. From many scheduling decisions we can learn more. Short BBs have very few scheduling decisions
    \item How: Count the number of instructions in the intermediate LLVM IR files of that basic block
\end{itemize}
\subsubsection{Most executed basic blocks}
\begin{itemize}
    \item What: The BB's that are executed the most often
    \item Why: Most important blocks. Good decisions here can lead to higher speedup
    \item How: Implement LLVM optimizer pass to inject counters into each BB. Execute to count. Generate List of counts
\end{itemize}
\todo{Show example numbers}
\subsubsection{Most executed and longest basic blocks}
\begin{itemize}
    \item What: Combine the two previous heuristics
    \item Why: Most executed, can still be very short BB's. Like initilization of for loops
    \item How: Multiply both numbers
\end{itemize}
\todo{Show example}

\section{Learning to schedule}
\subsection{Local MCTS model}
\label{subsec:approach:ml:mcts}
\subsection{Global model}
\label{subsec:approach:ml:global}

\section{Data generation}
\begin{enumerate}
    \item 
    \begin{itemize}
        \item What: Need to generate data from which we can learn
        \item Why: Dataset has only the code, we need som metric to learn from
        \item How: Execute the code and measure the metric
    \end{itemize}
    \item
    \begin{itemize}
        \item What: We choose to optimize for the runtime
        \item Why: That's what users in the most situations are interested in
        \item How: Execute the BB and measure the runtime
    \end{itemize}
\end{enumerate}

\subsection{Runtime measurement unit}
It would be optimal to just measure the the scheduled instructions with perfect precision and reliability.
\subsubsection{Basic Block}
\begin{itemize}
    \item What: Measuring the basic block itself is complicated
    \item Why: we need very precise time measurements
    \item How: the typical length of a basic block ranges from a hand full of instructions to a few dozens
    \todo{Plot basic block length distribution}
\end{itemize}
\subsubsection{Function}
\label{sec:approach:datageneration:runtime:function}
A specific basic block of a function might be executed multiple times during function execution.
In this situation, this is relevant in to different aspects.

% \begin{itemize}
%     \item What: Advantage: BB is executed multiple times, so its runtime is easier to measure
%     \item Why: 
%     \item How: 
% \end{itemize}
We are more interested in speedups, rather than precise execution times.


\begin{itemize}
    \item What: Measure the function which contains the basic block is bad in general
    \item Why: Different paths could be taken throught the function, e.g. if-else, loops
    \item How: 
\end{itemize}
\begin{itemize}
    \item What: In this case it is okay
    \item Why: the benchmarks are deterministic, each path is taken the same number of times between executions
    \item How: 
\end{itemize}
\subsubsection{Program}
\begin{itemize}
    \item What: Measure the execution time of the whole program
    \item Why: Easy, but unreliable because of IO operations and other noise (e.g. OS), we are only interested in a small fraction of the code
    \item How: 
    \todo{Show visualization}
\end{itemize}

\subsection{Runtime measurement methods}
\todo{Could add little experiment where we compare hardware timers vs OS access}
\subsubsection{Profiling}
\begin{itemize}
    \item What: Profilers are a bad choice for measuring precise runtimes
    \item Why: They are just not designed for it, they serve other purposes.
        Profilers are making snapshots of the running system to measure performance. 
        There is no measurement from point A to point B which leads to inaccuracies
    \item How: Read e.g. the paper of gperf \cite{graham1982gprof}
\end{itemize}
\subsubsection{Operating system methods}
% In linux(C): clock_gettime().
% Windows: QueryPerformanceCounter().
% C++: std::high_resolution_clock().
These might return different clocks depending on the used hardware.
Returns the time from the hardware the OS runs on.
Has overhead but also handles problems.
Typically best for measurements in microseconds range and longer.
\subsubsection{Hardware performance counters}
Accessed by assembly instructions.
% x86: 'rdtsc' Saves current value into EDX:EAX registers
% ARM32: mrc p15m, 0, \%0, c15, c12, 1
% AARCH64: mrs \%0, PMCCNTR_EL0
% Aurora: fencei; smir \%0, \%usrcc
Measurements in CPU cycles.
Lowest overhead available.
Access might be protected (Linux module required).
\subsection{Basic block isolation}
\subsubsection{Basic block extraction}
How to get the assembly (compilation process).
Problems: function calls (remove), jumps(only in last instr, remove), load/access (memory access, readress to stack variable)
\todo{Create table with removed instructions}
\subsubsection{Isolated basic block execution}
From extracted basic block create inline assembly with stack variable. Now executable from C/C++.
Warmup 100 runs. Timings 1000 runs. 
\subsection{Computing rewards from runtimes}
There are some outliers in the runtimes. Sort runtimes, cutoff lower and upper 5\% of the runtimes to remove outliers and compute avg of the rest.
What could be reasons for outliers?
Show some distributions of measurements to justify why we are throwing away data.

\section{Application of the learned model}
\label{sec:approach:ml-scheduler}

% Probably wrong place here, but compare approach with auto-tuning approach.
% Why not use auto-tuning? Search space size, and lack of generalization could be a reason

% \section{Breaking down the problem (maybe just chapter introduction)}
% \begin{itemize}
%     \item Schedulers run on basic blocks -> Select the most executed (hottest) BB's
% \end{itemize}

% \section{Experimentation Pipeline}
% Explain and illustrate the pipeline incl. injection of timers and counters

% \section{Benchmarking}
% \begin{itemize}
%     \item Benchmarking methods: Instrumentation, sampling -> only instrumentation makes sense here. Justify this with instrumentation vs. sampling results (our timing vs. perf timing)
%     \item Where to inject timer in BB DAG
%     \begin{itemize}
%         \item We only want to measure optimized code, but not too kleinteilig because of lacking accuracy. Problematic code is IO code
%         \item Time whole function?
%         \item Time only relevant BB's with surrounding ones (e.g., loop headers) -> Where exactly place the the timer
%         \item Provide data and examples to demonstrate decision making
%     \end{itemize} 
% \end{itemize}
% \subsection{Implementation}
% Injection of the \lstinline[language=C++]{std::chrono::high_resolution_clock}

% \section{Training setup}
% Compile with default scheduler and measure its runtime.
% Compile with our scheduler, execute the program, measure the runtime and use it to train the agent.

% \section{MCTS}
% \subsection{Tree Modeling}
% We probably want to model the interdependence between instructions. 
% But depending on the benchmark programs, only a small fraction of possible edges between instructions are available.

% \eg When in the benchmark a MOV was scheduled and after that, there are only SUB and ADD.
% We cannot schedule another MOV even though that might be the optimal schedule.

% This problem must be addressed somehow.
% There are different possibilities:
% \begin{itemize}
%     \item Declare all the states with their possible successors to different states.
%     The problem with this approach is that the number of states gets very high and most states are visited rarely, possibly only in the given benchmark/basic-block.
%     \item Probablistic Policy like in RL. (Does that exist for MCTS?)
%     This might solve the problem with unavailable edges, but it does not help with learning at all because the agent would have to learn which instructions are available, too.
%     Which is not what we want.
% \end{itemize}