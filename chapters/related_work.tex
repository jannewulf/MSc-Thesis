\chapter{Related Work}

\section{Instruction Scheduling}
\label{sec:rw:instruction-scheduling}
Scheduling problems appear in many fields.
This is why general scheduling is a topic where much research exists.
Also the research on instruction scheduling has a long history.

Algorithms exist that can generate perfect instruction schedules for simple situations with perfect information.
The requirements are met for architectures with only one functional unit and uniform instruction latencies.
The best-known algorithm in this field is the Sethi-Ullman labelling algorithm~\cite{sethi1970generation}, which also served as a base for other algorithms~\cite{proebsting1991linear}.

However, these conditions are not existent in modern processors.
In more complex situations, the instruction scheduling problem is NP-complex~\cite{hennessy1983postpass}.
Modern processors use pipelines to achieve instruction parallelism, see \Cref{sec:bg:cpu}.
Consequently, most instruction schedulers that are used nowadays are based on the list scheduling framework.
The algorithms, that follow this approach, are better able to generate instruction schedules for pipelined processors.
Heller~\cite{heller1961sequencing} published an early work on how to approach instruction scheduling for these processors.
Well-known is the work of Landskov~\cite{landskov1980local} for introducing the list scheduling framework.
% Much research on further developments of the list scheduling was published~\cite{bernstein1991global,gibbons1986efficient,hennessy1983postpass}.

As elaborated in \Cref{sec:bg:cpu}, the available information on instruction latencies is mostly uncertain.
The reasons are instruction-level parallelism and uncertain memory latencies.
One approach to approach this problem, is balanced scheduling~\cite{kerns1993balanced,lo1995improving}.
Another idea that was elaborated was to use stochastic instruction scheduling~\cite{schielke2000stochastic}.

Instruction scheduling typically works on a basic block level.
This also means that transitions between basic blocks are not scheduled to work well together.
However, there is research on extending the scope to greater regions~\cite{fisher1981trace,bernstein1991global,hwu1993superblock}.

The first work that combined data-driven methods with instruction scheduling was a patent by \citeauthor{tarsy1994method}~\cite{tarsy1994method}, filed in \citeyear{tarsy1994method}.
They used a simple perceptron to optimize weights that are used in heuristics, which are used in list scheduling.
\citeauthor{beaty1996using}~\cite{beaty1996using} published a work in \citeyear{beaty1996using}, in which they have used genetic algorithms to generate heuristcs.
Reinforcement learning in combination with rollouts were used for instruction scheduling by \citeauthor{mcgovern1999scheduling}~\cite{mcgovern1999scheduling,mcgovern2002building}.
Priority functions that would prefer one over another instruction when presented a pair were built with decision trees by \citeauthor{moss1998learning}~\cite{moss1998learning}.
\citeauthor{cavazos2004inducing} showed that instruction scheduling only makes a difference on some basic blocks.
They used decision trees for selecting basic blocks for instruction scheduling~\cite{cavazos2004inducing}.
\citeauthor{russell2006learning} used hand-crafted feature vectors and decision trees to create heuristics~\cite{russell2006learning} to improve instruction scheduling decisions.
A newer work in this field was published by \citeauthor{jain2019learning}~\cite{jain2019learning}.
They trained a neural network to imitate the instruction schedules by the GCC compiler.

\section{Register Allocation}
\label{sec:rw:register-allocation}
We have disussed the implications of the instruction scheduling phase on the register allocation \Cref{sec:bg:compilers:backend}.
This interdependence was also shown in by \citeauthor{goodman1988code}~\cite{goodman1988code}.
\citeauthor{lavrov1962store} showed the connection between the graph-coloring problem and register allocation and the NP-completeness~\cite{lavrov1962store}.
This knowledge was first implemented in a compiler by \citeauthor{chaitin1982register}~\cite{chaitin1982register}.

In the field of register allocation also appeared research that builds the connection to data-driven methods.
\citeauthor{das2019deep} used a deep learning approach to solve the graph coloring problem~\cite{das2019deep}.
The newer and naturally better fitting technique of graph neural networks was used by \citeauthor{lemos2019graph} to approach the graph coloring problem~\cite{lemos2019graph}. 

% \subsection*{Register Allocation for Intel Processor Graphics}\cite{chen2018register}

\section{Compiler Phase-Ordering}
%Static neural compiler optimization via deep reinforcement learning~\cite{mammadli2020static}\\
% - Uses only static information extracted from IR 
% - IR embedded by using ben2018neural
% - Use deep q learning
% - training is executed by running the modified IR and measure the speedup 
% - reward is defined as ln(T(s_t)/T(s_t+1)) with T being the runtime


\subsection*{Autophase: Compiler phase-ordering for hls with deep reinforcement learning}\cite{huang2019autophase}

\section{Code Representation}
For making use of data driven techniques in the area of compiler optimization, it is required to somehow extract features from the code to make it accessible for data driven algorithms.
Older works usually made use of approaches that used hand-tuned features.
\todo{Maybe add references used in https://chriscummins.cc/u/ed/phd-thesis.pdf (3.3.2.1)}

Recent works are inspired by the advances in the the field of \ac{nlp}, which are caused by neural networks and continuous distributed vectors (referred to as embeddings) \eg, word2vec~\cite{mikolov2013efficient}. 
Although, human language is different from codes of programming languages in many aspects, embeddings prove to be useful in code related tasks, too.

Code inputs may be used directly in a high-level programming language or in an \ac{ir} (\eg, LLVM-IR~\cite{LLVM:CGO04}).
The advantage of using an \ac{ir} is that it is independent of the source programming language and the target architecture.

%Overviews:
%\begin{itemize}
%    \item ProGraML Paper under Motivation
%    \item https://chriscummins.cc/u/ed/phd-thesis.pdf (3.3.2.1)
%    \item https://arxiv.org/pdf/1904.03061.pdf
%\end{itemize}

Most approaches for representing high-level language code use some sort of the \ac{ast} in combination with various learning mechanisms.
%code2vec: Learning distributed representations of code \cite{alon2019code2vec}
% - works on source code
% - sensitive to identifier names
Alon et al.~\cite{alon2019code2vec} used paths of the \ac{ast} in combination with a Attention Neural Network model.
Others have used the \ac{ast} in combination with Gated Graph Neural Networks~\cite{ye2020deep, allamanis2017learning}, with Support Vector Machines~\cite{park2012using} or with \ac{lstm} Networks for tree structures~\cite{dam2018deep}.
%\cite{ye2020deep} % Deep Program Structure Modeling Through Multi-RelationalGraph-based Learning, graph-based deep learning (Gated Graph Neural Networks), AST
%\cite{allamanis2017learning} % LEARNING  TO REPRESENT PROGRAMS WITH GRAPHS, graph-based deep learning (Gated Graph Neural Networks), AST
%\cite{dam2018deep} % A deep tree-based model for software defect prediction, AST, tree-LSTM
%\cite{park2012using} % Using Graph-Based ProgramCharacterization for Predictive Modeling, SVM

%Neural code comprehension: A learnable representation of code semantics \cite{ben2018neural}
% - Defines embedding space inst2vec
% - Encodes LLVM-IR, independent of source programming language
% - Leveraging data- and control flow (Contextual Flow Graphs)
% - Use RNN
% - Analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks
% - See description in ProGraML Paper under Motivation
With Neural Code Comprehension (inst2vec)~\cite{ben2018neural}, Ben-Nun et al. defined an embedding space for the LLVM-IR.
Relevant information to discover code semantics are data and control flow. 
To emphasize the semantics, the data and control flow are represented in a novel graph structure, called \acp{xfg}.
%Before building the \ac{xfg}, the LLVM-IR code is split into basic blocks, so diverging control flow is eliminated.
The context of an individual statement, with size $N$, is defined as the statement and its graph neighbors that are connected by a path of length $N$.
This statement is then mapped to its embedding by using the skip-gram model~\cite{mikolov2013distributed}, which are known to work good in \ac{nlp} tasks.
The \ac{xfg} captures features like data and control dependence's, instructions and data types, which are important for our task.

\subsection*{ProGraML: Graph-based Deep Learning for Program Optimization and Analysis}\cite{cummins2020programl}
%\begin{itemize}
%    \item approach is insensitive to identifier names and preserves operand order and type information
%    \item compared to inst2vec, it can do the same plus preserve operand order, important to distinguish non-commutative ops
%    \item represent programs as directed multigraphs where statements, identifiers, and immediate values are vertices, and relations between vertices are edge
%    \item encode IR into a graph which will be consumed by a Message Passing Neural Network to execute some task
%\end{itemize}
\todo{Write RW text for ProGraML}

\subsection*{Compiler-based graph representations for deep learning models of code}\cite{brauckmann2020compiler}
\todo{Find Paper PDF and write text}

%IR2Vec: A Flow Analysis based Scalable Infrastructure for Program Encodings \cite{keerthy2019ir2vec}
%\begin{itemize}
%    \item Abstracts away the width of the datatype
%\end{itemize}
IR2Vec~\cite{keerthy2019ir2vec} is another approach that maps an \ac{ir} to a embedding space.
\todo{Write roughly how IR2Vec works}[...]
However, the datatype size, which is important for code optimizations, is abstracted away during the embedding process.

\section{Applied Machine Learning on Code}
\subsection*{Ithemal Accurate, portable and fast basic block throughput estimation using deep neural networks}\cite{mendis2019ithemal}
\subsection*{NeuroVectorizer: End-to-End Vectorization with DeepReinforcement Learning}\cite{haj2020neurovectorizer}
\subsection*{From Loop Fusion to Kernel Fusion: A Domain-Specific Approach to Locality Optimization}\cite{qiao2019loop}
\subsection*{A Machine Learning Approach for Performance Prediction and Scheduling on Heterogeneous CPUs}\cite{nemirovsky2017machine}



