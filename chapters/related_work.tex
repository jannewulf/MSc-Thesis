\chapter{Related Work}
\label{sec:rw}
In this chapter, we survey the existing relevant literature covering this thesis' topics. 
For instruction scheduling (\Cref{sec:rw:instruction-scheduling}) and register allocation (\Cref{sec:rw:register-allocation}), we first discuss classical approaches and continue with newer data-driven advancements.
Further, we discuss some other relevant works in the fields of machine learning-based compiler optimizations, runtime estimation, code feature extraction, and machine learning approaches on other scheduling tasks (\Cref{sec:rw:other}).
These are interesting because we use methods like this in our approach or they are closely related.

\section{Instruction Scheduling}
\label{sec:rw:instruction-scheduling}
\subsection{Classical Approaches}
Scheduling problems appear in many fields where tasks with dependencies and a cost need to be ordered for execution.
Therefore, general scheduling is a topic with much existing research.
Also, the research on instruction scheduling has a long history.

Some algorithms can generate perfect instruction schedules for simple situations with perfect information.
Architectures with only one functional unit and uniform instruction latencies fulfill the requirements.
The best-known algorithms in this field are the Sethi-Ullman labeling algorithm~\cite{sethi1970generation} and the work by \citeauthor{proebsting1991linear}~\cite{proebsting1991linear}.

However, these conditions are not present in modern processors, as discussed in \Cref{sec:bg:cpu}.
In more complex situations, the instruction scheduling problem is NP-complete~\cite{hennessy1983postpass}.
Modern processors use multiple pipelines to achieve instruction parallelism, see \Cref{sec:bg:cpu}.
Consequently, most instruction schedulers used nowadays are based on the list scheduling framework, which \citeauthor{landskov1980local}~\cite{landskov1980local} proposed.
The algorithms that follow this approach are better able to generate instruction schedules for pipelined processors.
\citeauthor{heller1961sequencing}~\cite{heller1961sequencing} published early work on how to approach instruction scheduling for these processors.
Further research on developments of list scheduling was published in~\cite{bernstein1991global,gibbons1986efficient,hennessy1983postpass}.

As elaborated in \Cref{sec:bg:cpu}, the available information on instruction latencies is often unreliable due to instruction-level parallelism and unpredictable memory latencies caused by cache hierarchies.
One way to approach this problem is balanced scheduling~\cite{kerns1993balanced,lo1995improving}.
Another proposed method was to use stochastic instruction scheduling~\cite{schielke2000stochastic}.

Instruction scheduling typically works on a basic block level.
This means that instruction schedulers cannot schedule transitions across basic blocks.
However, there is research on extending the scope to larger regions.
\citeauthor{fisher1981trace}~\cite{fisher1981trace} selected code paths in functions for instruction scheduling.
\citeauthor{bernstein1991global}~\cite{bernstein1991global} define regions of strongly connected code (\eg loops) and execute scheduling on this unit.
Superblock scheduling was proposed by \citeauthor{hwu1993superblock}~\cite{hwu1993superblock}.
A superblock consists of multiple consecutive basic blocks and thus can start execution from other instructions than the first one.

\subsection{Data-Driven Approaches}
List schedulers usually have multiple heuristics used to choose instructions from the list of available instructions.
The selection is based on a weighted sum of the heuristics.
The first work that combined data-driven methods with instruction scheduling is a patent by \citeauthor{tarsy1994method}~\cite{tarsy1994method}, filed in \citeyear{tarsy1994method}.
They optimize weights used in cost-based heuristics.
These heuristics are used in list scheduling for pipelined processors.

Similarly, \citeauthor{beaty1996using}~\cite{beaty1996using} published a work in \citeyear{beaty1996using} in which they used a genetic algorithm to learn weights for different heuristics.
They achieved a 5\% performance increase compared to a random scheduler on three architectures.

\citeauthor{moss1997learning}~\cite{moss1997learning} trained a function that picks one instruction over another when presented the previously scheduled instructions.
They use decision trees, look-up tables, ELF function approximations, and feed-forward neural networks.
The decision tree performs best and often finds the optimal schedule.
However, they only use simulations and limit the basic block length to ten instructions.

\citeauthor{mcgovern1999scheduling}~\cite{mcgovern1999scheduling,mcgovern2002building} propose a reinforcement learning and a search heuristic.
Their reinforcement learning heuristic sometimes found a better instruction schedule than their baseline.
The long-running search approach found a better instruction schedule every time.
However, their baseline was only a random instruction scheduler, and they have only used simulation results instead of using a state-of-the-art compiler and executing their benchmarks on hardware.

\citeauthor{russell2006learning}~\cite{russell2006learning} uses decision trees to create heuristics for improving instruction scheduling decisions.
They show that they generate better instruction schedules 7.8 times more often than the compared heuristics.
They also evaluate their work only on a simulator.

A newer work in this field was published by \citeauthor{jain2019learning}~\cite{jain2019learning}.
They train a neural network to imitate the instruction schedules by the GCC compiler.
However, the performance of the GCC instruction scheduler limits this approachâ€™s performance.

We conclude that machine learning-based approaches mostly performed well in theory but only against weak random baselines.
The only work that we found that was actually evaluated on hardware was~\cite{beaty1996using}.

\section{Register Allocation}
\label{sec:rw:register-allocation}
We have discussed the implications of the instruction scheduling phase on register allocation in \Cref{sec:bg:compilers:backend}.
Their interdependence was also shown by \citeauthor{goodman1988code}~\cite{goodman1988code}.
\citeauthor{lavrov1962store}~\cite{lavrov1962store} showed the connection between the graph-coloring problem and register allocation and thus, the NP-completeness.
The first graph-coloring-based algorithm was implemented in a compiler by \citeauthor{chaitin1982register}~\cite{chaitin1982register}.

In the field of register allocation, also appeared research that builds the connection to data-driven methods.
\citeauthor{das2019deep}~\cite{das2019deep} use a deep learning approach to solve the graph coloring problem.
The newer and naturally better fitting approach with graph neural networks was used by \citeauthor{lemos2019graph}~\cite{lemos2019graph} to solve the graph coloring problem. 

\section{Related Areas}
\label{sec:rw:other}
\subsection{Compiler Optimizations with Machine Learning}
Machine learning approaches are also applied to optimize other parts of the compilation process.
\citeauthor{mammadli2020static}~\cite{mammadli2020static} and \citeauthor{huang2019autophase}~\cite{huang2019autophase} successfully apply deep reinforcement learning to the phase-ordering problem.
Phase-ordering means to select the compiler's optimization passes and define its execution order (see \Cref{sec:bg:compilers:optimizer} for information on the optimization phase).
Deep reinforcement learning was also used by \citeauthor{haj2020neurovectorizer}~\cite{haj2020neurovectorizer} to translate loops into vector processing instructions (SIMD).
\citeauthor{wang2009mapping}~\cite{wang2009mapping} used machine learning to predict the optimal number of threads and the optimal scheduling policy for OpenMP parallelized loops.
We refer to the surveys~\cite{wang2018machine,ashouri2018survey} for more literature.

\subsection{Runtime Estimation}
\label{sec:rw:other:runtime}
There are various tools for throughput and runtime estimation, like Ithemal~\cite{mendis2019ithemal}, llvm-mca\footnote{\url{https://llvm.org/docs/CommandGuide/llvm-mca.html}}, and Intel Architecture Code Analyzer (IACA)\footnote{\url{https://software.intel.com/content/www/us/en/develop/articles/intel-architecture-code-analyzer.html}}.
However, the listed tools only work with the x86 architecture, which we do not use.
The research projects and open-source estimators might be extended to other hardware architectures.
Especially the Ithemal~\cite{mendis2019ithemal} project is interesting as they use a neural network to predict the runtime from the basic block.
That means they learned to extract relevant features from the basic blocks instructions.

\subsection{Feature Extraction from Code}
The previously cited works on data-driven machine learning optimizations have or might benefit from research whose goal is to extract features from code automatically.
A similar approach to the word2vec~\cite{mikolov2013efficient} approach in the \ac{nlp} area was proposed by~\cite{ben2018neural,alon2019code2vec}.
\citeauthor{cummins2021programl}~\cite{cummins2021programl} developed a method to extract features from code based on graph structures.
The work proposed by \citeauthor{brauckmann2020compiler}~\cite{brauckmann2020compiler} works similarly -- they also work on graph structures and use graph neural networks to extract features.

\subsection{Other Scheduling Tasks with Machine Learning}
\citeauthor{mao2019learning}~\cite{mao2019learning} have used a deep reinforcement learning approach to schedule data-processing jobs onto computing clusters.
This work is interesting because the jobs have dependencies on each other, represented in a \ac{dag}, just like the instructions in the instruction scheduling problem.
